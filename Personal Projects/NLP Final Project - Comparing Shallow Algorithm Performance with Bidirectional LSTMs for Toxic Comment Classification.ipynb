{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <h1>IST 664 Final Project</h1>\n",
    "<h2>Professor Nancy McCracken\n",
    "    <br>\n",
    "Syracuse University\n",
    "    <br>\n",
    "By Mike Kane</h2>\n",
    "\n",
    "</center>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<center><h2> Toxic Comment Classification Using Bidirectional Neural Networks </center></h2>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from subprocess import check_output\n",
    "import sys\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import codecs\n",
    "import gensim\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import keras \n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Input, Dense, LSTM, GRU, Embedding\n",
    "from keras.layers import Dropout, Activation, Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk \n",
    "from nltk import word_tokenize\n",
    "from nltk.text import TextCollection\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = 'data/'\n",
    "train_path = '{}train.csv'.format(base_path)\n",
    "test_path = '{}test.csv'.format(base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(train_path)\n",
    "test_df = pd.read_csv(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_train_df = train_df.drop(['id', 'comment_text'], axis=1, inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "toxic            15294\n",
       "severe_toxic      1595\n",
       "obscene           8449\n",
       "threat             478\n",
       "insult            7877\n",
       "identity_hate     1405\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_train_df.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restructuring As a Binary Classification Problem\n",
    "\n",
    "The data set is currently set up as a multi-categorical problem to detect 6 different kinds of toxic comments. This function rehapes the data into a binary classification problem by merging all target columns into a single column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1], dtype=int64)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['target'] = train_df.drop(['id', 'comment_text'], axis=1, inplace=False).any(axis=1)\n",
    "train_df['target'] = train_df['target'].map({True: 1, False: 0})\n",
    "train_df['target'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0002bcb3da6cb337</td>\n",
       "      <td>COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0005c987bdfc9d4b</td>\n",
       "      <td>Hey... what is it..\\n@ | talk .\\nWhat is it......</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0007e25b2121310b</td>\n",
       "      <td>Bye! \\n\\nDon't look, come or think of comming ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>001810bf8c45bf5f</td>\n",
       "      <td>You are gay or antisemmitian? \\n\\nArchangel WH...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>00190820581d90ce</td>\n",
       "      <td>FUCK YOUR FILTHY MOTHER IN THE ASS, DRY!</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id                                       comment_text  \\\n",
       "6   0002bcb3da6cb337       COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK   \n",
       "12  0005c987bdfc9d4b  Hey... what is it..\\n@ | talk .\\nWhat is it......   \n",
       "16  0007e25b2121310b  Bye! \\n\\nDon't look, come or think of comming ...   \n",
       "42  001810bf8c45bf5f  You are gay or antisemmitian? \\n\\nArchangel WH...   \n",
       "43  00190820581d90ce           FUCK YOUR FILTHY MOTHER IN THE ASS, DRY!   \n",
       "\n",
       "    toxic  severe_toxic  obscene  threat  insult  identity_hate  target  \n",
       "6       1             1        1       0       1              0       1  \n",
       "12      1             0        0       0       0              0       1  \n",
       "16      1             0        0       0       0              0       1  \n",
       "42      1             0        1       0       1              1       1  \n",
       "43      1             0        1       0       1              0       1  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[train_df.target == 1].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        comment_text  target\n",
       "0  Explanation\\nWhy the edits made under my usern...       0\n",
       "1  D'aww! He matches this background colour I'm s...       0\n",
       "2  Hey man, I'm really not trying to edit war. It...       0\n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...       0\n",
       "4  You, sir, are my hero. Any chance you remember...       0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols_to_drop = ['id', 'toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "binarized_labels_df = train_df.drop(cols_to_drop, axis=1, inplace=False)\n",
    "binarized_labels_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of Positive Examples: 16225\n",
      "# of Negative Examples: 143346\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x2d15affbc18>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"# of Positive Examples: {}\".format(binarized_labels_df.target.sum()))\n",
    "print(\"# of Negative Examples: {}\".format(len(binarized_labels_df) - binarized_labels_df.target.sum()))\n",
    "binarized_labels_df.target.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reduce Sample Size\n",
    "\n",
    "As is, models run out of memory when training on the dataset. We'll train on a sample of the data to alleviate this error and reduce run times. Since there is class imbalance, we'll want to stratify our sampling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate positive and negative examples in training\n",
    "pos_train_df = binarized_labels_df[binarized_labels_df['target'] == 1]\n",
    "neg_train_df = binarized_labels_df[binarized_labels_df['target'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get number of positive examples\n",
    "type(len(pos_train_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate indices for random sampling of negative training examples\n",
    "neg_index = list(np.random.randint(0, high=len(neg_train_df)-1, size=len(pos_train_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>142997</th>\n",
       "      <td>193.1.217.20, You nailed it. Thanks for what y...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132144</th>\n",
       "      <td>\"\\n\\nUnblock: South Korean population update. ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97091</th>\n",
       "      <td>Hello\\nI just wanted to let you know how you c...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109396</th>\n",
       "      <td>definition? \\nso does stephen mean anything? s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35417</th>\n",
       "      <td>Afghanistan\\n\\nI just saw Matt Sanchez on the ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment_text  target\n",
       "142997  193.1.217.20, You nailed it. Thanks for what y...       0\n",
       "132144  \"\\n\\nUnblock: South Korean population update. ...       0\n",
       "97091   Hello\\nI just wanted to let you know how you c...       0\n",
       "109396  definition? \\nso does stephen mean anything? s...       0\n",
       "35417   Afghanistan\\n\\nI just saw Matt Sanchez on the ...       0"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Downsample Data by slicing the DataFrame using the random integers generated as indices\n",
    "neg_samples_df = pd.DataFrame(neg_train_df.iloc[neg_index], columns=neg_train_df.columns)\n",
    "neg_samples_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16225"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(neg_samples_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32450\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>193.1.217.20, You nailed it. Thanks for what y...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"\\n\\nUnblock: South Korean population update. ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hello\\nI just wanted to let you know how you c...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>definition? \\nso does stephen mean anything? s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Afghanistan\\n\\nI just saw Matt Sanchez on the ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        comment_text  target\n",
       "0  193.1.217.20, You nailed it. Thanks for what y...       0\n",
       "1  \"\\n\\nUnblock: South Korean population update. ...       0\n",
       "2  Hello\\nI just wanted to let you know how you c...       0\n",
       "3  definition? \\nso does stephen mean anything? s...       0\n",
       "4  Afghanistan\\n\\nI just saw Matt Sanchez on the ...       0"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.concat([neg_samples_df, pos_train_df])\n",
    "print(len(train_df))\n",
    "train_df.index = range(0, len(train_df))\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Step 1: Tokenize All Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_comment(c):\n",
    "    stopset = set(stopwords.words('english') + list(string.punctuation) + [\"''\", '\"\"', '``'])\n",
    "    tokens = nltk.word_tokenize(c)\n",
    "    stopwords_removed = [t.lower() for t in tokens if t not in stopset]\n",
    "    return stopwords_removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['tokenized_comments'] = train_df['comment_text'].map(cleanup_comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['193.1.217.20', 'you', 'nailed', 'thanks', 'wrote']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.tokenized_comments[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Store Labels and Data Separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = train_df.tokenized_comments\n",
    "labels = train_df.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Get Total Vocabulary\n",
    "\n",
    "To get the total vocabulary, we'll combine everything into a set. During this step, we'll also concatenate all comments and use NLTK.FreqDist() to retrieve the top 25000 most common words. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78709"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_vocab = set()\n",
    "for comment in data:\n",
    "    total_vocab.update(comment)\n",
    "len(total_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once stop words and punctuation have been removed, there are 78,709 words in this data set. \n",
    "\n",
    "Now, we'll create a frequency distribution and get the top 2000 most common words, for testing purposes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_comments_combined = \"\"\n",
    "for comment in data:\n",
    "    for word in comment:\n",
    "        all_comments_combined += word + \" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_freq_dist = nltk.FreqDist(cleanup_comment(all_comments_combined))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_words = comments_freq_dist.most_common(2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Vectorization\n",
    "\n",
    "We'll start by using the Bag of Words/Unigram baseline used in labs from class. This creates a vector of booleans, where each element in vector corresponds to one of the 2000 unique words in the `most_common_words` frequency distribution we created above. If the word is present in the comment, then the element corresponding to that word will be `True`, otherwise `False`. These will be extremely sparse vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_features(document, word_features):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['V_{}'.format(word)] = (word in document_words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresets = [document_features(comment, most_common_words) for comment in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresets = pd.DataFrame(featuresets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Creating Training and Testing Sets\n",
    "\n",
    "Here, we would normally split our data into training and testing sets. However, since we're using cross validation, we don't actually need to do this. The `cross_val_score` function from sklearn will handle this for us. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(featuresets, labels, test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Train Classifiers and Check Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "base_rf = RandomForestClassifier(n_estimators=100)\n",
    "base_xgboost = XGBClassifier(jobs=-1)\n",
    "base_bernoulli_nb = BernoulliNB()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5, 0.5, 0.5])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cross Validation Score for Random Forest\n",
    "cross_val_score(base_rf, featuresets, labels, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5, 0.5, 0.5])"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(base_xgboost, featuresets, labels, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5, 0.5, 0.5])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(base_bernoulli_nb, featuresets, labels, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_write = featuresets\n",
    "to_write['labels'] = labels\n",
    "to_write.to_csv(\"top_10000_words.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "None of the models did better than random chance when the vocab size was reduced to the 2000 most common words. Each scored a mean Cross-Validation accuracy of 50%. \n",
    "\n",
    "I experimented with different vocabulary sizes, and the results were as follows.\n",
    "\n",
    "|       Vocab Size      | Random Forest  | XGBoost | Bernoulli Naive Bayes |\n",
    "|:-----------:|:--------------:|:-------:|:---------------------:|\n",
    "|  2000 Words |       0.5      |   0.5   |          0.5          |\n",
    "| 5000 words |         0.5       |   0.5      |     0.5                  |\n",
    "| 10000 words |       Memory Error         |   Memory Error      |    Memory Error                   |\n",
    "\n",
    "When I tried creating a version of the dataset with the top 10000 words, I got a memory error when trying to create the dataset. Upon inspection, a CSV of the transformed dataset (and corresponding labels) clocks in at just under a gigabyte, so it makes sense that the 10,000 word version wouldn't fit in memory. In it's current naive format, this is too big, but this dataset's dimensionality shouldnt be a problem once we use one of the major libraries. As is, the dataset wastes a lot of space that can be optimized once we take into account that these are extremely sparse vectors. \n",
    "\n",
    "\n",
    "## Part 2: Using Word2Vec Vectorization\n",
    "\n",
    "Since we had no success with the current wa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Section\n",
    "\n",
    "This task is best suited for a Deep Learning approach. We have enough data, and the need for context suggests an embedding layer will be very helpful for capturing context and nuance. Furthermore, the ability of LSTMs to learn to remember what's important (and forget what isn't!) will likely be very helpful in helping deal with the complexities of the language in the comments. \n",
    "\n",
    "### Preprocessing Steps\n",
    "\n",
    "To set up the data for use in our model, we'll make use of the `preprocessing` module found within `keras`. Specifically, we'll use the following packages:\n",
    "\n",
    "**_Text_**: The `text` module provides access to keras's own _Tokenizer_ class. We'll use this tokenizer to fit on the dataset, and then use it to convert the text to sequences, which are the input type we'll need to use to feed our data into embedding and LSTM layers. \n",
    "\n",
    "**_sequence_**: Once we've created the sequences, we'll then use the `sequence` class to pad all sequences, so that they are all the same uniform shape. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"data/train.csv\")\n",
    "test = pd.read_csv(\"data/test.csv\")\n",
    "train = train.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_sentences_train = train[\"comment_text\"].fillna(\"CVxTz\").values\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train[list_classes].values\n",
    "list_sentences_test = test[\"comment_text\"].fillna(\"CVxTz\").values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = text.Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "X_t = sequence.pad_sequences(list_tokenized_train, maxlen=100)\n",
    "X_te = sequence.pad_sequences(list_tokenized_test, maxlen=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture\n",
    "\n",
    "For the Deep Learning model, we'll use a **_Bidrectional LSTM_**, with a **_Multi Layer Perceptron_** tacked on to the end to handle the actual classification. The output of the model is a vector with 6 elements, all bounded between 0 and 1. Each element corresponds to a different kind of toxic comment (in order, \"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", and \"identity_hate\"). Although this is a multiclass classification problem, the classes are **_not mutually exclusive_**, meaning that a comment can be multiple kinds of toxic classes at the same time. \n",
    "\n",
    "\n",
    "The architecture we'll begin with is as follows:\n",
    "\n",
    "1. An **_Embedding Layer_** that will act as a Word2Vec model that creates word embeddings based on the text docudments fed into the model.  This will allow the model to vectorize the text in an advanced way that captures both contextual references, as well as common noun phrases (e.g. treating \"New York\" as a single phrase, instead of individual \"New\" and \"York\" tokens).  \n",
    "<br>  \n",
    "2. A **_Bidrectional Layer_** containing an **_LSTM_** layer consisting of 50 cells. This bidirectional layer handles all the complexity of bidirectional learning. Let's assume that the network is trying to classify a sentence containing 10 words, with we can label each word $X_t$ sequentially as $X_0, X_1, ... X_9$, respectively. Half of the bidirectional layer will work as a traditional feed-forward RNN, starting at $X_0$, making a prediction, and then passing along it's internal state and its prediction along to the next time step, along with word $X_1$, and so on until it reaches the end of the sequence. The other half of the model does the same things, but starts at the end and works backwards towards the beginning--$X_9$, then $X_8$, and so on.    \n",
    "<br>     \n",
    "\n",
    "3. In both make predictions at the same time, and then we use a **_Global MaxPooling_** operation to pool each time step and take the strongest prediction at each time step, regardless of whether it came from the front-to-back or back-to-front portion of the Bidirectional LSTM.  \n",
    "<br>  \n",
    "   \n",
    "4. We then tack on a **_Dropout Layer_** to regularize our network and discourage overfitting, with each neuron's \"keep chance\" at 90%.   \n",
    "<br>    \n",
    "   \n",
    "5. Next, we have a Dense layer to learn feature representations based on what was returned from the Bidirectional LSTM layer. Everything between the first and last Dense layers can basically be interpreted as a vanilla feed-forward neural network--the only difference being that the first hidden Dense layer is learning from the sequences returned by the Bidrectional LSTM layer, rather than a traditional input layer. As with the previous layer, another **_Dropout Layer_** is used for regularization purposes.   \n",
    "<br>    \n",
    "    \n",
    "6. The final layer of the network is a fully-connected layer with 6 neurons, where each corresponds to one of the 6 kinds of toxic classes the model is trying to predict. The choice of activation function here is a bit unique for a multiclass classification problem. Typically, multiclass classification problems utilize a softmax activation function, which splits total likelihood of 1. across each element in the vector (with the element with the highest value being the class predicted by the model). However, this classification problem is unique in that any given example can multiple kinds of target classes at the same time. For example, a toxic comment that is both \"obscene\", 'threatening' and 'severe_toxic'. Because of this, each of the neurons uses a sigmoid activation function, allowing the model to act as 6 different binary classifiers at the same time--1 for each of the target classes. \n",
    "<br>  \n",
    "\n",
    "## Other Notes\n",
    "\n",
    "### Embedding Size Parameter\n",
    "The embedding size is heuristically determined (meaning it's a magic number I found recommended online for this problem). \n",
    "\n",
    "### Callbacks\n",
    "\n",
    "Since the model is **_extremely slow_** to train, I've utilized two callbacks:\n",
    "\n",
    "**_Weight Checkpoints:_**  A checkpoint callback for recording the weights for the model's best performance at any given time, so that we can always reload them easily and save on training time. \n",
    "\n",
    "**_Early Stopping Checkpoint:_** A checkpoint that utilizes early stopping if model performance stagnates, or begins to drop due to overfitting on the training set. \n",
    "\n",
    "### Compile-Time Parameters\n",
    "\n",
    "I used the following compile-time parameters for this model:\n",
    "\n",
    "**_Loss: Binary Crossentropy_**. As mentioned above, although the problem is multiclass in nature, the special case of overlapping classes means that we need to use _Binary Crossentropy_ as our loss function to compute the loss for each neuron in the output layer. The _sigmoid_ activation functions on the neurons in the output layer make this a dead giveaway (whereas if we had used a _softmax_ activation function, we'd want to make use of _Categorical Crossentropy_ instead).\n",
    "\n",
    "**_Optimizer: adam_**. I elected to choose the _adam_ optimizer, which is short for **_Adaptive Moment Estimation_**. This was another heuristic choice, although it tends to be my go-to optimizer for models that take a long time to train, as it's currently considered the most advanced optimizer out there. The _adam_ optimizer is a combination of the _RMSProp_ optimizer (short for _Root Mean Square Propagation_) and _Nesterov Momentum_. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 128\n",
    "input_ = Input(shape=(100,))\n",
    "x = Embedding(15000, embedding_size)(input_)\n",
    "x = Bidirectional(LSTM(50, return_sequences=True))(x)\n",
    "x = GlobalMaxPool1D()(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(50, activation='relu')(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(6, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inputs = input_, outputs=x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"556pt\" viewBox=\"0.00 0.00 231.00 556.00\" width=\"231pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 552)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"#ffffff\" points=\"-4,4 -4,-552 227,-552 227,4 -4,4\" stroke=\"transparent\"/>\n",
       "<!-- 113142322344 -->\n",
       "<g class=\"node\" id=\"node1\">\n",
       "<title>113142322344</title>\n",
       "<polygon fill=\"none\" points=\"61,-511.5 61,-547.5 162,-547.5 162,-511.5 61,-511.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"111.5\" y=\"-525.8\">input_1: InputLayer</text>\n",
       "</g>\n",
       "<!-- 113142323744 -->\n",
       "<g class=\"node\" id=\"node2\">\n",
       "<title>113142323744</title>\n",
       "<polygon fill=\"none\" points=\"47.5,-438.5 47.5,-474.5 175.5,-474.5 175.5,-438.5 47.5,-438.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"111.5\" y=\"-452.8\">embedding_1: Embedding</text>\n",
       "</g>\n",
       "<!-- 113142322344&#45;&gt;113142323744 -->\n",
       "<g class=\"edge\" id=\"edge1\">\n",
       "<title>113142322344-&gt;113142323744</title>\n",
       "<path d=\"M111.5,-511.4551C111.5,-503.3828 111.5,-493.6764 111.5,-484.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"115.0001,-484.5903 111.5,-474.5904 108.0001,-484.5904 115.0001,-484.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 112289834208 -->\n",
       "<g class=\"node\" id=\"node3\">\n",
       "<title>112289834208</title>\n",
       "<polygon fill=\"none\" points=\"6,-365.5 6,-401.5 217,-401.5 217,-365.5 6,-365.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"111.5\" y=\"-379.8\">bidirectional_1(lstm_1): Bidirectional(LSTM)</text>\n",
       "</g>\n",
       "<!-- 113142323744&#45;&gt;112289834208 -->\n",
       "<g class=\"edge\" id=\"edge2\">\n",
       "<title>113142323744-&gt;112289834208</title>\n",
       "<path d=\"M111.5,-438.4551C111.5,-430.3828 111.5,-420.6764 111.5,-411.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"115.0001,-411.5903 111.5,-401.5904 108.0001,-411.5904 115.0001,-411.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 113142332160 -->\n",
       "<g class=\"node\" id=\"node4\">\n",
       "<title>113142332160</title>\n",
       "<polygon fill=\"none\" points=\"0,-292.5 0,-328.5 223,-328.5 223,-292.5 0,-292.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"111.5\" y=\"-306.8\">global_max_pooling1d_1: GlobalMaxPooling1D</text>\n",
       "</g>\n",
       "<!-- 112289834208&#45;&gt;113142332160 -->\n",
       "<g class=\"edge\" id=\"edge3\">\n",
       "<title>112289834208-&gt;113142332160</title>\n",
       "<path d=\"M111.5,-365.4551C111.5,-357.3828 111.5,-347.6764 111.5,-338.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"115.0001,-338.5903 111.5,-328.5904 108.0001,-338.5904 115.0001,-338.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 113142304440 -->\n",
       "<g class=\"node\" id=\"node5\">\n",
       "<title>113142304440</title>\n",
       "<polygon fill=\"none\" points=\"61.5,-219.5 61.5,-255.5 161.5,-255.5 161.5,-219.5 61.5,-219.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"111.5\" y=\"-233.8\">dropout_1: Dropout</text>\n",
       "</g>\n",
       "<!-- 113142332160&#45;&gt;113142304440 -->\n",
       "<g class=\"edge\" id=\"edge4\">\n",
       "<title>113142332160-&gt;113142304440</title>\n",
       "<path d=\"M111.5,-292.4551C111.5,-284.3828 111.5,-274.6764 111.5,-265.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"115.0001,-265.5903 111.5,-255.5904 108.0001,-265.5904 115.0001,-265.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 113792197520 -->\n",
       "<g class=\"node\" id=\"node6\">\n",
       "<title>113792197520</title>\n",
       "<polygon fill=\"none\" points=\"70,-146.5 70,-182.5 153,-182.5 153,-146.5 70,-146.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"111.5\" y=\"-160.8\">dense_1: Dense</text>\n",
       "</g>\n",
       "<!-- 113142304440&#45;&gt;113792197520 -->\n",
       "<g class=\"edge\" id=\"edge5\">\n",
       "<title>113142304440-&gt;113792197520</title>\n",
       "<path d=\"M111.5,-219.4551C111.5,-211.3828 111.5,-201.6764 111.5,-192.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"115.0001,-192.5903 111.5,-182.5904 108.0001,-192.5904 115.0001,-192.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 113792100616 -->\n",
       "<g class=\"node\" id=\"node7\">\n",
       "<title>113792100616</title>\n",
       "<polygon fill=\"none\" points=\"61.5,-73.5 61.5,-109.5 161.5,-109.5 161.5,-73.5 61.5,-73.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"111.5\" y=\"-87.8\">dropout_2: Dropout</text>\n",
       "</g>\n",
       "<!-- 113792197520&#45;&gt;113792100616 -->\n",
       "<g class=\"edge\" id=\"edge6\">\n",
       "<title>113792197520-&gt;113792100616</title>\n",
       "<path d=\"M111.5,-146.4551C111.5,-138.3828 111.5,-128.6764 111.5,-119.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"115.0001,-119.5903 111.5,-109.5904 108.0001,-119.5904 115.0001,-119.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 113793081128 -->\n",
       "<g class=\"node\" id=\"node8\">\n",
       "<title>113793081128</title>\n",
       "<polygon fill=\"none\" points=\"70,-.5 70,-36.5 153,-36.5 153,-.5 70,-.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"111.5\" y=\"-14.8\">dense_2: Dense</text>\n",
       "</g>\n",
       "<!-- 113792100616&#45;&gt;113793081128 -->\n",
       "<g class=\"edge\" id=\"edge7\">\n",
       "<title>113792100616-&gt;113793081128</title>\n",
       "<path d=\"M111.5,-73.4551C111.5,-65.3828 111.5,-55.6764 111.5,-46.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"115.0001,-46.5903 111.5,-36.5904 108.0001,-46.5904 115.0001,-46.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints_path = 'weights_base.best.hdf5'\n",
    "checkpoint = ModelCheckpoint(checkpoints_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', mode='min', patience=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [checkpoint, early_stopping]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/2\n",
      "143613/143613 [==============================] - 1341s 9ms/step - loss: 0.0639 - acc: 0.9790 - val_loss: 0.0485 - val_acc: 0.9825\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.04850, saving model to weights_base.best.hdf5\n",
      "Epoch 2/2\n",
      "143613/143613 [==============================] - 1339s 9ms/step - loss: 0.0460 - acc: 0.9830 - val_loss: 0.0476 - val_acc: 0.9834\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.04850 to 0.04756, saving model to weights_base.best.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2d171284780>"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_t, y, batch_size=32, epochs=2, validation_split=0.1, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ = Input(shape=(100,))\n",
    "x = Embedding(15000, embedding_size)(input_)\n",
    "x = Bidirectional(LSTM(75, return_sequences=True))(x)\n",
    "x = GlobalMaxPool1D()(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(75, activation='relu')(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(6, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inputs = input_, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints_path_2 = 'weights_base_iter2.best.hdf5'\n",
    "checkpoint_2 = ModelCheckpoint(checkpoints_path_2, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "early_stopping_2 = EarlyStopping(monitor='val_loss', mode='min', patience=20)\n",
    "callbacks_2 = [checkpoint_2, early_stopping_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/2\n",
      "143613/143613 [==============================] - 1377s 10ms/step - loss: 0.0667 - acc: 0.9781 - val_loss: 0.0516 - val_acc: 0.9814\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.05156, saving model to weights_base_iter2.best.hdf5\n",
      "Epoch 2/2\n",
      "143613/143613 [==============================] - 1355s 9ms/step - loss: 0.0473 - acc: 0.9827 - val_loss: 0.0468 - val_acc: 0.9834\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.05156 to 0.04678, saving model to weights_base_iter2.best.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2d1713c3518>"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_t, y, batch_size=32, epochs=2, validation_split=0.1, callbacks=callbacks_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  # Conclusion\n",
    "  \n",
    "  The LSTM model was able to quite easily perform toxic comment classification. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
